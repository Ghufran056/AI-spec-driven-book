"use strict";(globalThis.webpackChunkai_driven_book=globalThis.webpackChunkai_driven_book||[]).push([[4662],{4982:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapter6/6.4-manipulation-pipeline","title":"Manipulation Pipeline","description":"Learning Objectives","source":"@site/docs/chapter6/6.4-manipulation-pipeline.mdx","sourceDirName":"chapter6","slug":"/chapter6/6.4-manipulation-pipeline","permalink":"/AI-spec-driven-book/docs/chapter6/6.4-manipulation-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter6/6.4-manipulation-pipeline.mdx","tags":[],"version":"current","frontMatter":{"id":"6.4-manipulation-pipeline","title":"Manipulation Pipeline","sidebar_label":"6.4 - Manipulation Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"6.3 - Grasp Planning Strategies","permalink":"/AI-spec-driven-book/docs/chapter6/6.3-grasp-planning-strategies"},"next":{"title":"6.5 - MoveIt Motion Planning Basics","permalink":"/AI-spec-driven-book/docs/chapter6/6.5-moveit-motion-planning-basics"}}');var a=t(4848),o=t(8453);const r={id:"6.4-manipulation-pipeline",title:"Manipulation Pipeline",sidebar_label:"6.4 - Manipulation Pipeline"},s="Manipulation Pipeline",l={},p=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Main Theory",id:"main-theory",level:2},{value:"1. Perception-Action Cycle",id:"1-perception-action-cycle",level:3},{value:"2. Sensor Integration",id:"2-sensor-integration",level:3},{value:"3. Task and Motion Planning",id:"3-task-and-motion-planning",level:3},{value:"4. Feedback Control",id:"4-feedback-control",level:3},{value:"5. Failure Detection and Recovery",id:"5-failure-detection-and-recovery",level:3},{value:"6. Human-Robot Interaction",id:"6-human-robot-interaction",level:3},{value:"Examples",id:"examples",level:2},{value:"Example: Complete Manipulation Pipeline Implementation",id:"example-complete-manipulation-pipeline-implementation",level:3},{value:"Example: Error Handling and Recovery in Manipulation",id:"example-error-handling-and-recovery-in-manipulation",level:3},{value:"Example: Integration with ROS/ROS 2",id:"example-integration-with-rosros-2",level:3},{value:"Practical Notes",id:"practical-notes",level:2},{value:"Summary",id:"summary",level:2},{value:"Glossary",id:"glossary",level:2},{value:"Quick Quiz",id:"quick-quiz",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"manipulation-pipeline",children:"Manipulation Pipeline"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand the complete workflow of a robotic manipulation system"}),"\n",(0,a.jsx)(n.li,{children:"Identify the key components and their interactions in a manipulation pipeline"}),"\n",(0,a.jsx)(n.li,{children:"Design and implement a basic manipulation pipeline for simple tasks"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the performance and robustness of manipulation systems"}),"\n",(0,a.jsx)(n.li,{children:"Integrate perception, planning, and control components effectively"}),"\n",(0,a.jsx)(n.li,{children:"Recognize common failure modes and recovery strategies in manipulation"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"A robotic manipulation pipeline is a systematic workflow that integrates perception, planning, and control components to enable robots to interact with objects in their environment. The pipeline typically begins with sensing the environment, followed by object recognition and pose estimation, grasp planning, motion planning, and finally execution with feedback control. Each component in the pipeline affects the performance of subsequent components, making the design and integration of these systems critical for successful manipulation."}),"\n",(0,a.jsx)(n.p,{children:"The manipulation pipeline must handle uncertainty at each stage, from sensor noise in perception to execution errors in control. Modern manipulation systems often include recovery mechanisms and re-planning capabilities to handle failures and adapt to changing conditions. The pipeline needs to operate in real-time while maintaining safety and reliability, especially when interacting with humans or delicate objects."}),"\n",(0,a.jsx)(n.p,{children:"The design of manipulation pipelines varies depending on the application requirements, robot hardware, and environmental constraints. Some pipelines are highly specialized for specific tasks, while others are designed to be general-purpose and adaptable to various manipulation scenarios. Understanding the flow of information and the dependencies between components is essential for developing robust manipulation systems."}),"\n",(0,a.jsx)(n.h2,{id:"main-theory",children:"Main Theory"}),"\n",(0,a.jsx)(n.h3,{id:"1-perception-action-cycle",children:"1. Perception-Action Cycle"}),"\n",(0,a.jsx)(n.p,{children:"The fundamental loop in manipulation systems where perception informs action, and action affects the environment which is then perceived again, creating a continuous cycle."}),"\n",(0,a.jsx)(n.h3,{id:"2-sensor-integration",children:"2. Sensor Integration"}),"\n",(0,a.jsx)(n.p,{children:"Combining data from multiple sensors (cameras, depth sensors, force/torque sensors) to create a comprehensive understanding of the manipulation scene."}),"\n",(0,a.jsx)(n.h3,{id:"3-task-and-motion-planning",children:"3. Task and Motion Planning"}),"\n",(0,a.jsx)(n.p,{children:"Hierarchical planning approach that combines high-level task planning with low-level motion planning for complex manipulation sequences."}),"\n",(0,a.jsx)(n.h3,{id:"4-feedback-control",children:"4. Feedback Control"}),"\n",(0,a.jsx)(n.p,{children:"Continuous monitoring and adjustment of robot actions based on sensor feedback to handle uncertainties and disturbances during manipulation."}),"\n",(0,a.jsx)(n.h3,{id:"5-failure-detection-and-recovery",children:"5. Failure Detection and Recovery"}),"\n",(0,a.jsx)(n.p,{children:"Mechanisms to detect when manipulation attempts fail and implement recovery strategies to continue the task."}),"\n",(0,a.jsx)(n.h3,{id:"6-human-robot-interaction",children:"6. Human-Robot Interaction"}),"\n",(0,a.jsx)(n.p,{children:"Integration of human guidance and collaboration into the manipulation pipeline for shared control scenarios."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Complete Manipulation Pipeline:\n\n[Environment] \u2192 [Perception] \u2192 [Scene Understanding] \u2192 [Task Planning] \u2192 [Motion Planning] \u2192 [Control] \u2192 [Action]\n      \u2191              \u2191                  \u2191                   \u2191                \u2191            \u2191         \u2191\n[Objects] \u2190\u2192 [Sensors] \u2190\u2192 [Object Models] \u2190\u2192 [Grasp Planning] \u2190\u2192 [Trajectory] \u2190\u2192 [Actuators] \u2190\u2192 [Effectors]\n"})}),"\n",(0,a.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(n.h3,{id:"example-complete-manipulation-pipeline-implementation",children:"Example: Complete Manipulation Pipeline Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport time\nfrom typing import List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ManipulationState(Enum):\n    IDLE = "idle"\n    PERCEIVING = "perceiving"\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    RECOVERING = "recovering"\n    COMPLETED = "completed"\n    FAILED = "failed"\n\n@dataclass\nclass ManipulationGoal:\n    """Defines what the manipulation system should achieve"""\n    object_name: str\n    target_position: np.ndarray\n    target_orientation: Optional[np.ndarray] = None\n    grasp_type: str = "power"\n\n@dataclass\nclass ManipulationResult:\n    """Result of a manipulation attempt"""\n    success: bool\n    message: str\n    execution_time: float\n    grasp_quality: float = 0.0\n\nclass PerceptionModule:\n    """Handles object detection and pose estimation"""\n\n    def __init__(self):\n        self.detected_objects = {}\n\n    def process_sensor_data(self, rgb_image, depth_image):\n        """Process sensor data to detect objects"""\n        # In practice, this would use computer vision algorithms\n        # For this example, we\'ll simulate object detection\n\n        # Simulate detecting a "red_cube" at a specific position\n        detected_objects = {\n            "red_cube": {\n                "position": np.array([0.5, 0.0, 0.1]),  # meters\n                "orientation": np.array([0, 0, 0]),      # Euler angles\n                "confidence": 0.95\n            },\n            "blue_cylinder": {\n                "position": np.array([0.6, 0.2, 0.1]),\n                "orientation": np.array([0, 0, 0]),\n                "confidence": 0.88\n            }\n        }\n\n        self.detected_objects = detected_objects\n        return detected_objects\n\n    def get_object_pose(self, object_name: str) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n        """Get the pose of a specific object"""\n        if object_name in self.detected_objects:\n            obj_data = self.detected_objects[object_name]\n            return obj_data["position"], obj_data["orientation"]\n        return None\n\nclass GraspPlanner:\n    """Plans how to grasp objects"""\n\n    def __init__(self):\n        self.min_grasp_quality = 0.6\n\n    def plan_grasp(self, object_pose: Tuple[np.ndarray, np.ndarray],\n                   object_name: str) -> Optional[dict]:\n        """Plan a grasp for the given object"""\n        position, orientation = object_pose\n\n        # Simple grasp planning based on object position\n        # In practice, this would use more sophisticated algorithms\n\n        # Calculate approach and grasp poses\n        approach_offset = np.array([0, 0, 0.1])  # Approach from above\n        approach_pose = position + approach_offset\n\n        grasp_pose = position.copy()\n\n        # Calculate grasp quality based on object properties\n        grasp_quality = self.estimate_grasp_quality(object_name)\n\n        if grasp_quality >= self.min_grasp_quality:\n            return {\n                "approach_pose": approach_pose,\n                "grasp_pose": grasp_pose,\n                "pre_grasp_pose": position + np.array([0, 0, 0.02]),  # 2cm above\n                "grasp_quality": grasp_quality,\n                "success": True\n            }\n\n        return {"success": False, "message": "Low grasp quality"}\n\n    def estimate_grasp_quality(self, object_name: str) -> float:\n        """Estimate grasp quality based on object properties"""\n        # Simple estimation based on object type\n        if "cube" in object_name.lower():\n            return 0.8\n        elif "cylinder" in object_name.lower():\n            return 0.7\n        else:\n            return 0.6\n\nclass MotionPlanner:\n    """Plans robot trajectories for manipulation"""\n\n    def __init__(self):\n        self.collision_threshold = 0.02  # 2cm clearance\n\n    def plan_trajectory(self, start_pose: np.ndarray,\n                       waypoints: List[np.ndarray]) -> Optional[List[np.ndarray]]:\n        """Plan a collision-free trajectory through waypoints"""\n        # In practice, this would use motion planning algorithms like RRT or PRM\n\n        # For this example, we\'ll create a simple linear interpolation\n        # between waypoints with collision checking\n        trajectory = [start_pose]\n\n        for waypoint in waypoints:\n            # Simple linear interpolation (in practice, use proper path planning)\n            steps = 10\n            for i in range(1, steps + 1):\n                t = i / steps\n                interpolated_pose = start_pose * (1 - t) + waypoint * t\n                trajectory.append(interpolated_pose)\n\n            start_pose = waypoint\n\n        # Check for collisions (simplified)\n        if self.check_collision_free(trajectory):\n            return trajectory\n        else:\n            return None\n\n    def check_collision_free(self, trajectory: List[np.ndarray]) -> bool:\n        """Check if trajectory is collision-free"""\n        # Simplified collision checking\n        # In practice, this would check against environment geometry\n        for pose in trajectory:\n            # Check if pose is in valid workspace (simplified)\n            if not (-1.0 <= pose[0] <= 1.0 and\n                    -1.0 <= pose[1] <= 1.0 and\n                    0.0 <= pose[2] <= 1.5):\n                return False\n        return True\n\nclass Controller:\n    """Controls robot execution with feedback"""\n\n    def __init__(self):\n        self.max_velocity = 0.1  # m/s\n        self.max_acceleration = 0.5  # m/s^2\n        self.gripper_force_limit = 50.0  # N\n\n    def execute_trajectory(self, trajectory: List[np.ndarray]) -> bool:\n        """Execute the planned trajectory"""\n        # In practice, this would interface with robot hardware\n        # For simulation, we\'ll just process the trajectory\n\n        success = True\n        for i, target_pose in enumerate(trajectory):\n            # Simulate movement to target pose\n            time.sleep(0.01)  # Simulate execution time\n\n            # Check for execution success (simplified)\n            if np.random.random() > 0.99:  # 1% failure rate for demonstration\n                success = False\n                break\n\n        return success\n\n    def execute_grasp(self, grasp_pose: np.ndarray,\n                     grasp_quality: float) -> bool:\n        """Execute grasp action"""\n        # Simulate grasp execution\n        success_probability = min(0.95, grasp_quality + 0.1)  # Add some uncertainty\n        return np.random.random() < success_probability\n\nclass ManipulationPipeline:\n    """Complete manipulation pipeline integrating all components"""\n\n    def __init__(self):\n        self.perception = PerceptionModule()\n        self.grasp_planner = GraspPlanner()\n        self.motion_planner = MotionPlanner()\n        self.controller = Controller()\n        self.state = ManipulationState.IDLE\n        self.current_goal: Optional[ManipulationGoal] = None\n\n    def execute_manipulation(self, goal: ManipulationGoal) -> ManipulationResult:\n        """Execute the complete manipulation pipeline"""\n        start_time = time.time()\n        self.current_goal = goal\n        self.state = ManipulationState.PERCEIVING\n\n        try:\n            # Step 1: Perception\n            print("Step 1: Perception")\n            detected_objects = self.perception.process_sensor_data(None, None)\n\n            if goal.object_name not in detected_objects:\n                return ManipulationResult(\n                    success=False,\n                    message=f"Object \'{goal.object_name}\' not found",\n                    execution_time=time.time() - start_time\n                )\n\n            # Step 2: Grasp Planning\n            print("Step 2: Grasp Planning")\n            object_pose = self.perception.get_object_pose(goal.object_name)\n            grasp_plan = self.grasp_planner.plan_grasp(object_pose, goal.object_name)\n\n            if not grasp_plan["success"]:\n                return ManipulationResult(\n                    success=False,\n                    message=grasp_plan.get("message", "Grasp planning failed"),\n                    execution_time=time.time() - start_time\n                )\n\n            # Step 3: Motion Planning\n            print("Step 3: Motion Planning")\n            current_robot_pose = np.array([0.0, 0.0, 0.5])  # Starting position\n\n            # Define waypoints for the manipulation task\n            waypoints = [\n                grasp_plan["approach_pose"],\n                grasp_plan["pre_grasp_pose"],\n                grasp_plan["grasp_pose"]\n            ]\n\n            trajectory = self.motion_planner.plan_trajectory(current_robot_pose, waypoints)\n\n            if trajectory is None:\n                return ManipulationResult(\n                    success=False,\n                    message="Motion planning failed - no collision-free path found",\n                    execution_time=time.time() - start_time\n                )\n\n            # Step 4: Execution\n            print("Step 4: Execution")\n            self.state = ManipulationState.EXECUTING\n\n            # Execute the approach trajectory\n            if not self.controller.execute_trajectory(trajectory[:-1]):  # All but grasp pose\n                return ManipulationResult(\n                    success=False,\n                    message="Trajectory execution failed during approach",\n                    execution_time=time.time() - start_time\n                )\n\n            # Execute the grasp\n            grasp_success = self.controller.execute_grasp(\n                grasp_plan["grasp_pose"],\n                grasp_plan["grasp_quality"]\n            )\n\n            if not grasp_success:\n                return ManipulationResult(\n                    success=False,\n                    message="Grasp execution failed",\n                    execution_time=time.time() - start_time,\n                    grasp_quality=grasp_plan["grasp_quality"]\n                )\n\n            # If we reach here, manipulation was successful\n            self.state = ManipulationState.COMPLETED\n            return ManipulationResult(\n                success=True,\n                message="Manipulation completed successfully",\n                execution_time=time.time() - start_time,\n                grasp_quality=grasp_plan["grasp_quality"]\n            )\n\n        except Exception as e:\n            self.state = ManipulationState.FAILED\n            return ManipulationResult(\n                success=False,\n                message=f"Manipulation failed with error: {str(e)}",\n                execution_time=time.time() - start_time\n            )\n\n# Example usage\ndef pipeline_example():\n    pipeline = ManipulationPipeline()\n\n    # Define a manipulation goal\n    goal = ManipulationGoal(\n        object_name="red_cube",\n        target_position=np.array([0.7, 0.3, 0.1]),\n        grasp_type="power"\n    )\n\n    # Execute the manipulation\n    result = pipeline.execute_manipulation(goal)\n\n    print(f"\\nManipulation Result:")\n    print(f"  Success: {result.success}")\n    print(f"  Message: {result.message}")\n    print(f"  Time: {result.execution_time:.3f}s")\n    print(f"  Grasp Quality: {result.grasp_quality:.3f}")\n\nif __name__ == "__main__":\n    pipeline_example()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"example-error-handling-and-recovery-in-manipulation",children:"Example: Error Handling and Recovery in Manipulation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport time\nfrom enum import Enum\nfrom typing import List, Optional\n\nclass RecoveryStrategy(Enum):\n    RETRY = "retry"\n    REGRASP = "regrasp"\n    REPLAN = "replan"\n    ABORT = "abort"\n\nclass RecoveryManager:\n    """Manages error recovery in manipulation pipeline"""\n\n    def __init__(self):\n        self.max_retries = 3\n        self.current_attempts = 0\n        self.failure_history = []\n\n    def handle_failure(self, failure_type: str, context: dict) -> RecoveryStrategy:\n        """Determine recovery strategy based on failure type and context"""\n        self.failure_history.append({\n            \'type\': failure_type,\n            \'time\': time.time(),\n            \'context\': context\n        })\n\n        self.current_attempts += 1\n\n        # Different recovery strategies based on failure type\n        if failure_type == "grasp_failure":\n            if self.current_attempts <= 2:\n                return RecoveryStrategy.REGRASP  # Try different grasp\n            else:\n                return RecoveryStrategy.REPLAN  # Plan different approach\n\n        elif failure_type == "motion_failure":\n            if self.current_attempts <= 1:\n                return RecoveryStrategy.REPLAN  # Try different path\n            else:\n                return RecoveryStrategy.ABORT\n\n        elif failure_type == "perception_failure":\n            if self.current_attempts <= 2:\n                return RecoveryStrategy.RETRY  # Retake sensor data\n            else:\n                return RecoveryStrategy.ABORT\n\n        else:\n            if self.current_attempts <= self.max_retries:\n                return RecoveryStrategy.RETRY\n            else:\n                return RecoveryStrategy.ABORT\n\n    def reset(self):\n        """Reset recovery manager for new task"""\n        self.current_attempts = 0\n        self.failure_history = []\n\nclass RobustManipulationPipeline:\n    """Manipulation pipeline with error handling and recovery"""\n\n    def __init__(self):\n        self.perception = PerceptionModule()\n        self.grasp_planner = GraspPlanner()\n        self.motion_planner = MotionPlanner()\n        self.controller = Controller()\n        self.recovery_manager = RecoveryManager()\n        self.max_recovery_attempts = 3\n\n    def execute_with_recovery(self, goal):\n        """Execute manipulation with automatic recovery from failures"""\n        attempt_count = 0\n\n        while attempt_count < self.max_recovery_attempts:\n            print(f"\\nAttempt {attempt_count + 1} of {self.max_recovery_attempts}")\n\n            result = self._execute_single_attempt(goal)\n\n            if result.success:\n                return result\n\n            print(f"Attempt failed: {result.message}")\n\n            # Determine recovery strategy\n            recovery_strategy = self.recovery_manager.handle_failure(\n                "general_failure",\n                {"attempt": attempt_count, "result": result}\n            )\n\n            print(f"Recovery strategy: {recovery_strategy.value}")\n\n            if recovery_strategy == RecoveryStrategy.ABORT:\n                return ManipulationResult(\n                    success=False,\n                    message=f"Manipulation failed after {attempt_count + 1} attempts",\n                    execution_time=result.execution_time\n                )\n\n            # Apply recovery strategy\n            if recovery_strategy == RecoveryStrategy.RETRY:\n                print("Retrying same approach...")\n                time.sleep(0.5)  # Brief pause before retry\n            elif recovery_strategy == RecoveryStrategy.REGRASP:\n                print("Planning new grasp...")\n                # In a real system, this would involve new grasp planning\n                pass\n            elif recovery_strategy == RecoveryStrategy.REPLAN:\n                print("Replanning entire approach...")\n                # In a real system, this would involve complete replanning\n                pass\n\n            attempt_count += 1\n\n        return ManipulationResult(\n            success=False,\n            message=f"Manipulation failed after {self.max_recovery_attempts} recovery attempts",\n            execution_time=time.time() - time.time()  # Placeholder for actual time\n        )\n\n    def _execute_single_attempt(self, goal):\n        """Execute a single manipulation attempt"""\n        # This would contain the same logic as the basic pipeline\n        # but is simplified here for brevity\n        start_time = time.time()\n\n        # Simulate potential failure\n        if np.random.random() < 0.3:  # 30% failure rate for demonstration\n            return ManipulationResult(\n                success=False,\n                message="Simulated failure in single attempt",\n                execution_time=time.time() - start_time\n            )\n\n        # Simulate success\n        return ManipulationResult(\n            success=True,\n            message="Success in single attempt",\n            execution_time=time.time() - start_time\n        )\n\n# Example with recovery\ndef recovery_example():\n    pipeline = RobustManipulationPipeline()\n\n    goal = ManipulationGoal(\n        object_name="red_cube",\n        target_position=np.array([0.7, 0.3, 0.1])\n    )\n\n    result = pipeline.execute_with_recovery(goal)\n    print(f"\\nFinal result: {result.message}")\n    print(f"Success: {result.success}")\n\nif __name__ == "__main__":\n    recovery_example()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"example-integration-with-rosros-2",children:"Example: Integration with ROS/ROS 2"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# This would typically be implemented as a ROS 2 node\n# For demonstration, we\'ll show the conceptual structure\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nfrom manipulation_msgs.msg import ManipulationGoal, ManipulationResult\n\nclass ManipulationPipelineROS(Node):\n    """ROS 2 node implementing the manipulation pipeline"""\n\n    def __init__(self):\n        super().__init__(\'manipulation_pipeline\')\n\n        # Create subscribers for sensor data\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            \'/camera/depth/image_raw\',\n            self.depth_callback,\n            10\n        )\n\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2,\n            \'/camera/depth/color/points\',\n            self.pointcloud_callback,\n            10\n        )\n\n        # Create publishers for results\n        self.result_pub = self.create_publisher(\n            ManipulationResult,\n            \'/manipulation/result\',\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            \'/manipulation/status\',\n            10\n        )\n\n        # Create action server for manipulation goals\n        # This would use the ROS 2 action interface\n        self.current_goal = None\n\n        # Initialize internal pipeline components\n        self.pipeline = ManipulationPipeline()\n\n        # Timer for processing loop\n        self.processing_timer = self.create_timer(0.1, self.process_callback)\n\n        self.get_logger().info(\'Manipulation Pipeline node initialized\')\n\n    def image_callback(self, msg):\n        """Handle incoming RGB image data"""\n        # Process image and update internal state\n        self.get_logger().debug(\'Received RGB image\')\n\n    def depth_callback(self, msg):\n        """Handle incoming depth image data"""\n        self.get_logger().debug(\'Received depth image\')\n\n    def pointcloud_callback(self, msg):\n        """Handle incoming point cloud data"""\n        self.get_logger().debug(\'Received point cloud\')\n\n    def process_callback(self):\n        """Main processing loop"""\n        # This would contain the main pipeline logic\n        # checking for new goals, processing data, etc.\n        pass\n\ndef ros_main():\n    """Main function to run the ROS manipulation pipeline"""\n    rclpy.init()\n    manipulation_node = ManipulationPipelineROS()\n\n    try:\n        rclpy.spin(manipulation_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        manipulation_node.destroy_node()\n        rclpy.shutdown()\n\n# The actual ROS implementation would be more complex\n# and involve action servers, service calls, and proper\n# integration with robot hardware interfaces\n'})}),"\n",(0,a.jsx)(n.h2,{id:"practical-notes",children:"Practical Notes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement robust error handling and recovery mechanisms"}),"\n",(0,a.jsx)(n.li,{children:"Consider real-time performance requirements for each pipeline component"}),"\n",(0,a.jsx)(n.li,{children:"Validate pipeline components individually before integration"}),"\n",(0,a.jsx)(n.li,{children:"Test pipeline under various environmental conditions"}),"\n",(0,a.jsx)(n.li,{children:"Monitor and log pipeline performance for debugging"}),"\n",(0,a.jsx)(n.li,{children:"Plan for graceful degradation when components fail"}),"\n",(0,a.jsx)(n.li,{children:"Consider human oversight and intervention capabilities"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"The manipulation pipeline integrates perception, planning, and control components into a cohesive system for robotic manipulation. Success requires careful design of each component and their interactions, with particular attention to error handling and recovery. Modern manipulation systems must operate reliably in uncertain environments while maintaining safety and efficiency. The pipeline serves as the foundation for complex robotic manipulation tasks in various applications."}),"\n",(0,a.jsx)(n.h2,{id:"glossary",children:"Glossary"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Manipulation Pipeline"}),": Integrated system of perception, planning, and control for robot manipulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception-Action Cycle"}),": Continuous loop where perception informs action and action affects perception"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Task Planning"}),": High-level planning of manipulation sequence and subtasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Motion Planning"}),": Low-level planning of robot trajectories and movements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Grasp Planning"}),": Determining how to grasp objects effectively"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feedback Control"}),": Continuous adjustment of robot actions based on sensor feedback"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Recovery Strategy"}),": Plan for handling and recovering from manipulation failures"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Collision Checking"}),": Verifying robot trajectories don't collide with obstacles"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Grasp Quality"}),": Metric for evaluating the stability of a grasp"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Force Control"}),": Controlling the forces applied during manipulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Impedance Control"}),": Controlling robot's mechanical impedance during interaction"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Shared Control"}),": Human-robot collaboration in manipulation tasks"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"quick-quiz",children:"Quick Quiz"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What are the three main components of a manipulation pipeline?\nA) Perception, Planning, Control\nB) Sensing, Actuation, Communication\nC) Detection, Grasping, Moving\nD) Vision, Mechanics, Electronics"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What is the purpose of a perception-action cycle in manipulation?\nA) To reduce computational requirements\nB) To create a continuous loop where perception informs action and action affects the environment\nC) To increase robot speed\nD) To reduce sensor requirements"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Which of the following is NOT a typical component of a manipulation pipeline?\nA) Perception module\nB) Motion planning module\nC) Task planning module\nD) Network routing module"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"What is the main purpose of recovery strategies in manipulation systems?\nA) To increase manipulation speed\nB) To handle failures and continue tasks when possible\nC) To reduce computational load\nD) To improve sensor accuracy"}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:'What does "grasp quality" refer to in manipulation systems?\nA) The visual appearance of the grasp\nB) A metric for evaluating the stability and effectiveness of a grasp\nC) The type of gripper used\nD) The speed of the grasp'}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Answers:"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"A) Perception, Planning, Control"}),"\n",(0,a.jsx)(n.li,{children:"B) To create a continuous loop where perception informs action and action affects the environment"}),"\n",(0,a.jsx)(n.li,{children:"D) Network routing module"}),"\n",(0,a.jsx)(n.li,{children:"B) To handle failures and continue tasks when possible"}),"\n",(0,a.jsx)(n.li,{children:"B) A metric for evaluating the stability and effectiveness of a grasp"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);