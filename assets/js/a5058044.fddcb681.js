"use strict";(globalThis.webpackChunkai_driven_book=globalThis.webpackChunkai_driven_book||[]).push([[772],{8031:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter2/2.3-camera-systems-and-stereo-vision","title":"Camera Systems and Stereo Vision","description":"Learning Objectives","source":"@site/docs/chapter2/2.3-camera-systems-and-stereo-vision.mdx","sourceDirName":"chapter2","slug":"/chapter2/2.3-camera-systems-and-stereo-vision","permalink":"/AI-spec-driven-book/docs/chapter2/2.3-camera-systems-and-stereo-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter2/2.3-camera-systems-and-stereo-vision.mdx","tags":[],"version":"current","frontMatter":{"id":"2.3-camera-systems-and-stereo-vision","title":"Camera Systems and Stereo Vision","sidebar_label":"2.3 - Camera Systems and Stereo Vision"},"sidebar":"tutorialSidebar","previous":{"title":"2.2 - LIDAR and Depth Sensors","permalink":"/AI-spec-driven-book/docs/chapter2/2.2-lidar-and-depth-sensors"},"next":{"title":"2.4 - IMU, Balance, and Motion Tracking","permalink":"/AI-spec-driven-book/docs/chapter2/2.4-imu-balance-and-motion-tracking"}}');var s=i(4848),o=i(8453);const r={id:"2.3-camera-systems-and-stereo-vision",title:"Camera Systems and Stereo Vision",sidebar_label:"2.3 - Camera Systems and Stereo Vision"},a="Camera Systems and Stereo Vision",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"How it Works",id:"how-it-works",level:2},{value:"1. Pinhole Camera Model",id:"1-pinhole-camera-model",level:3},{value:"2. Stereo Vision Geometry",id:"2-stereo-vision-geometry",level:3},{value:"3. Image Formation Process",id:"3-image-formation-process",level:3},{value:"4. Color Sensing",id:"4-color-sensing",level:3},{value:"5. Stereo Disparity Calculation",id:"5-stereo-disparity-calculation",level:3},{value:"6. Depth Map Generation",id:"6-depth-map-generation",level:3},{value:"Simple Diagrams",id:"simple-diagrams",level:2},{value:"Real-world Examples",id:"real-world-examples",level:2},{value:"Autonomous Vehicles",id:"autonomous-vehicles",level:3},{value:"Robot Navigation",id:"robot-navigation",level:3},{value:"Object Manipulation",id:"object-manipulation",level:3},{value:"Depth Estimation and Stereo Disparity",id:"depth-estimation-and-stereo-disparity",level:3},{value:"Summary",id:"summary",level:2},{value:"Glossary",id:"glossary",level:2},{value:"Quick Quiz",id:"quick-quiz",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"camera-systems-and-stereo-vision",children:"Camera Systems and Stereo Vision"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the fundamental principles of camera-based vision in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Explain how stereo vision creates depth perception from two cameras"}),"\n",(0,s.jsx)(n.li,{children:"Identify different types of camera systems used in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Describe the process of stereo disparity and depth estimation"}),"\n",(0,s.jsx)(n.li,{children:"Recognize practical applications of camera systems in robotic perception"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Camera systems are among the most important sensors for robots that need to understand their visual environment. Unlike range sensors that provide distance measurements, cameras capture rich visual information that allows robots to recognize objects, read text, detect colors, and understand complex scenes. Stereo vision takes this a step further by using two or more cameras to estimate depth, providing 3D information similar to human vision."}),"\n",(0,s.jsx)(n.p,{children:"Robots use cameras for a wide variety of tasks including navigation, object recognition, quality inspection, and human-robot interaction. The visual information from cameras can be processed using computer vision algorithms to extract meaningful information about the environment. Stereo vision systems add depth perception to this visual information, enabling robots to understand the 3D structure of their environment."}),"\n",(0,s.jsx)(n.h2,{id:"how-it-works",children:"How it Works"}),"\n",(0,s.jsx)(n.h3,{id:"1-pinhole-camera-model",children:"1. Pinhole Camera Model"}),"\n",(0,s.jsx)(n.p,{children:"The basic model for understanding how cameras work, where light rays pass through a small aperture to form an inverted image. This model helps explain perspective projection and the relationship between 3D world points and 2D image points."}),"\n",(0,s.jsx)(n.h3,{id:"2-stereo-vision-geometry",children:"2. Stereo Vision Geometry"}),"\n",(0,s.jsx)(n.p,{children:"Two cameras separated by a known distance (the baseline) capture slightly different views of the same scene. The difference in position of objects between the two images (disparity) is inversely related to the object's distance from the cameras."}),"\n",(0,s.jsx)(n.h3,{id:"3-image-formation-process",children:"3. Image Formation Process"}),"\n",(0,s.jsx)(n.p,{children:"Light enters the camera through a lens, which focuses it onto an image sensor. The sensor converts light intensity into electrical signals that are processed to form a digital image. Calibration is needed to account for lens distortion and other factors."}),"\n",(0,s.jsx)(n.h3,{id:"4-color-sensing",children:"4. Color Sensing"}),"\n",(0,s.jsx)(n.p,{children:"Digital cameras use sensors with color filters (typically RGB) to capture color information. Different color spaces (RGB, HSV, etc.) are used for different applications depending on the requirements for color processing."}),"\n",(0,s.jsx)(n.h3,{id:"5-stereo-disparity-calculation",children:"5. Stereo Disparity Calculation"}),"\n",(0,s.jsx)(n.p,{children:"The process of finding corresponding points in left and right images and calculating the difference in their positions. This disparity is used to calculate depth using triangulation principles."}),"\n",(0,s.jsx)(n.h3,{id:"6-depth-map-generation",children:"6. Depth Map Generation"}),"\n",(0,s.jsx)(n.p,{children:"Converting stereo image pairs into depth maps where each pixel value represents the distance to the corresponding point in the scene. This creates a 2D representation of 3D depth information."}),"\n",(0,s.jsx)(n.h2,{id:"simple-diagrams",children:"Simple Diagrams"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Pinhole Camera Model:\n\n3D Scene Point\n      *\n      |\\\\\n      | \\\\\n      |  \\\\\n      |   \\\\ Image Plane\n      |    \\\\\n      |     \\\\\n      |      O (Pinhole)\n      |      |\n      |      | Image Point\n      |      *\n      |\n      Camera Center\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Stereo Vision Principle:\n\nLeft Camera O-------------------O Right Camera\n           \\\\                   /\n            \\\\                 /\n             \\\\               /\n              \\\\             /\n               \\\\           /\n                \\\\         /\n                 \\\\       /\n                  \\\\     /\n                   \\\\   /\n                    \\\\ /\n                     * Scene Point\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Stereo Disparity Example:\n\nLeft Image:     [Object]---------[Object]\n                * * *             * * *\n\nRight Image:    [Object]---------[Object]\n                   * * *             * * *\n\nDisparity:      <<<-- Distance between\n                  corresponding points\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Depth Estimation Formula:\n\nDepth = (Baseline \xd7 Focal Length) / Disparity\n\nWhere:\n- Baseline: Distance between the two cameras\n- Focal Length: Camera focal length\n- Disparity: Difference in pixel positions between images\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-world-examples",children:"Real-world Examples"}),"\n",(0,s.jsx)(n.h3,{id:"autonomous-vehicles",children:"Autonomous Vehicles"}),"\n",(0,s.jsx)(n.p,{children:"Self-driving cars use stereo camera systems to detect and measure the distance to other vehicles, pedestrians, and obstacles. Tesla's Autopilot system relies heavily on camera-based perception, using multiple cameras to create a comprehensive view of the environment."}),"\n",(0,s.jsx)(n.h3,{id:"robot-navigation",children:"Robot Navigation"}),"\n",(0,s.jsx)(n.p,{children:"Robots like the PR2 and other research platforms use stereo vision to navigate complex environments. The depth information allows them to identify obstacles, plan safe paths, and avoid collisions."}),"\n",(0,s.jsx)(n.h3,{id:"object-manipulation",children:"Object Manipulation"}),"\n",(0,s.jsx)(n.p,{children:"Robots in warehouses and manufacturing use stereo vision to locate and grasp objects. The depth information helps them determine the exact position and orientation of objects, enabling precise manipulation."}),"\n",(0,s.jsx)(n.h3,{id:"depth-estimation-and-stereo-disparity",children:"Depth Estimation and Stereo Disparity"}),"\n",(0,s.jsx)(n.p,{children:"Modern smartphones use stereo cameras or structured light systems to create depth maps for portrait mode photography. Similarly, robotics applications use stereo disparity to understand the 3D structure of scenes for navigation and manipulation tasks."}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Camera systems and stereo vision provide robots with rich visual information and depth perception capabilities. The combination of 2D visual data with 3D depth information enables robots to recognize objects, navigate safely, and interact with their environment effectively. Stereo vision systems use the geometric relationship between two cameras to calculate depth, providing 3D information from 2D images. As camera technology continues to improve, we're seeing more sophisticated visual perception capabilities in robotic systems."}),"\n",(0,s.jsx)(n.h2,{id:"glossary",children:"Glossary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Vision"}),": A method of depth perception using two or more cameras to calculate distance based on parallax"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disparity"}),": The difference in position of an object between left and right stereo images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pinhole Camera Model"}),": A simplified model of camera operation based on light rays passing through a point aperture"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Baseline"}),": The distance between the optical centers of two stereo cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Map"}),": A 2D image where pixel values represent distance to objects in the scene"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Calibration"}),": The process of determining camera parameters to correct for distortion and other optical effects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Triangulation"}),": A method of determining distance by measuring angles from two different positions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Focal Length"}),": The distance between the camera lens and the image sensor when the subject is in focus"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-quiz",children:"Quick Quiz"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What is stereo disparity?\nA) The color difference between two images\nB) The difference in position of an object between left and right stereo images\nC) The brightness difference between two images\nD) The size difference between two images"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In stereo vision, how is depth related to disparity?\nA) Depth is directly proportional to disparity\nB) Depth is inversely related to disparity\nC) Depth and disparity are unrelated\nD) Depth is the square of disparity"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What does the baseline refer to in stereo vision?\nA) The focal length of the camera\nB) The distance between the optical centers of two stereo cameras\nC) The height of the camera setup\nD) The width of the captured image"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Which formula correctly represents the relationship for depth estimation in stereo vision?\nA) Depth = Baseline + Focal Length + Disparity\nB) Depth = (Baseline \xd7 Focal Length) / Disparity\nC) Depth = Disparity / (Baseline \xd7 Focal Length)\nD) Depth = Baseline \xd7 Disparity \xd7 Focal Length"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What is a depth map?\nA) A map showing the colors of objects in a scene\nB) A 2D image where pixel values represent distance to objects in the scene\nC) A map showing the brightness of objects\nD) A map showing the texture of objects"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Answers:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"B) The difference in position of an object between left and right stereo images"}),"\n",(0,s.jsx)(n.li,{children:"B) Depth is inversely related to disparity"}),"\n",(0,s.jsx)(n.li,{children:"B) The distance between the optical centers of two stereo cameras"}),"\n",(0,s.jsx)(n.li,{children:"B) Depth = (Baseline \xd7 Focal Length) / Disparity"}),"\n",(0,s.jsx)(n.li,{children:"B) A 2D image where pixel values represent distance to objects in the scene"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);