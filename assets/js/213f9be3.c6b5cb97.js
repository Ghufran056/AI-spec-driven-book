"use strict";(globalThis.webpackChunkai_driven_book=globalThis.webpackChunkai_driven_book||[]).push([[1378],{7495:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"chapter5/5.6-synthetic-data-generation","title":"Synthetic Data Generation","description":"Learning Objectives","source":"@site/docs/chapter5/5.6-synthetic-data-generation.mdx","sourceDirName":"chapter5","slug":"/chapter5/5.6-synthetic-data-generation","permalink":"/AI-spec-driven-book/docs/chapter5/5.6-synthetic-data-generation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter5/5.6-synthetic-data-generation.mdx","tags":[],"version":"current","frontMatter":{"id":"5.6-synthetic-data-generation","title":"Synthetic Data Generation","sidebar_label":"5.6 - Synthetic Data Generation"},"sidebar":"tutorialSidebar","previous":{"title":"5.5 - Nav2 Navigation in Isaac","permalink":"/AI-spec-driven-book/docs/chapter5/5.5-nav2-navigation-in-isaac"},"next":{"title":"5.7 - Isaac for Humanoid Robotics","permalink":"/AI-spec-driven-book/docs/chapter5/5.7-isaac-for-humanoid-robotics"}}');var i=a(4848),r=a(8453);const o={id:"5.6-synthetic-data-generation",title:"Synthetic Data Generation",sidebar_label:"5.6 - Synthetic Data Generation"},s="Synthetic Data Generation",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Main Theory",id:"main-theory",level:2},{value:"1. Synthetic Data Fundamentals",id:"1-synthetic-data-fundamentals",level:3},{value:"2. Domain Randomization",id:"2-domain-randomization",level:3},{value:"3. Procedural Generation",id:"3-procedural-generation",level:3},{value:"4. Physics-Based Rendering",id:"4-physics-based-rendering",level:3},{value:"5. Annotation Generation",id:"5-annotation-generation",level:3},{value:"6. Reality Gap Mitigation",id:"6-reality-gap-mitigation",level:3},{value:"Examples",id:"examples",level:2},{value:"Example: Isaac Sim Synthetic Data Generation",id:"example-isaac-sim-synthetic-data-generation",level:3},{value:"Example: Domain Randomization Configuration",id:"example-domain-randomization-configuration",level:3},{value:"Example: Synthetic Data Quality Evaluation",id:"example-synthetic-data-quality-evaluation",level:3},{value:"Practical Notes",id:"practical-notes",level:2},{value:"Summary",id:"summary",level:2},{value:"Glossary",id:"glossary",level:2},{value:"Quick Quiz",id:"quick-quiz",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"synthetic-data-generation",children:"Synthetic Data Generation"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand the principles and applications of synthetic data generation in robotics"}),"\n",(0,i.jsx)(e.li,{children:"Implement synthetic data generation pipelines using simulation environments"}),"\n",(0,i.jsx)(e.li,{children:"Apply domain randomization techniques to improve model generalization"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate the quality and realism of synthetic data for robotics applications"}),"\n",(0,i.jsx)(e.li,{children:"Integrate synthetic data generation with machine learning workflows"}),"\n",(0,i.jsx)(e.li,{children:"Recognize the advantages and limitations of synthetic data vs. real data"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Synthetic data generation is the process of creating artificial data using simulation environments, computer graphics, and procedural algorithms rather than collecting it from real-world sources. In robotics, synthetic data generation has become increasingly important as it addresses the challenge of obtaining large, diverse, and well-annotated datasets required for training machine learning models. Simulation environments like Isaac Sim provide photorealistic rendering capabilities and accurate physics simulation, making it possible to generate high-quality synthetic data that can be used to train perception, navigation, and control systems."}),"\n",(0,i.jsx)(e.p,{children:"The primary advantage of synthetic data is the ability to generate unlimited amounts of data with perfect annotations, including ground truth labels, depth maps, segmentation masks, and other annotations that would be expensive or impossible to obtain from real data. Synthetic data generation also allows for the creation of rare or dangerous scenarios that would be difficult to capture in real-world data collection, such as emergency situations or edge cases."}),"\n",(0,i.jsx)(e.p,{children:"Synthetic data generation is particularly valuable in robotics applications where real-world data collection is time-consuming, expensive, or dangerous. By leveraging simulation environments, robotics developers can create diverse training datasets that include various lighting conditions, environmental variations, and object configurations that help improve the robustness and generalization of their machine learning models."}),"\n",(0,i.jsx)(e.h2,{id:"main-theory",children:"Main Theory"}),"\n",(0,i.jsx)(e.h3,{id:"1-synthetic-data-fundamentals",children:"1. Synthetic Data Fundamentals"}),"\n",(0,i.jsx)(e.p,{children:"Synthetic data generation involves creating artificial datasets using simulation, computer graphics, and procedural generation techniques to produce data that mimics real-world observations."}),"\n",(0,i.jsx)(e.h3,{id:"2-domain-randomization",children:"2. Domain Randomization"}),"\n",(0,i.jsx)(e.p,{children:"Technique of randomizing visual and physical properties in simulation to improve the transfer of models trained on synthetic data to real-world applications."}),"\n",(0,i.jsx)(e.h3,{id:"3-procedural-generation",children:"3. Procedural Generation"}),"\n",(0,i.jsx)(e.p,{children:"Method of creating content algorithmically rather than manually, allowing for the generation of diverse and varied synthetic environments and objects."}),"\n",(0,i.jsx)(e.h3,{id:"4-physics-based-rendering",children:"4. Physics-Based Rendering"}),"\n",(0,i.jsx)(e.p,{children:"Rendering techniques that simulate the physical behavior of light to create realistic synthetic images that match real-world conditions."}),"\n",(0,i.jsx)(e.h3,{id:"5-annotation-generation",children:"5. Annotation Generation"}),"\n",(0,i.jsx)(e.p,{children:"Process of automatically generating ground truth annotations (labels, segmentation, depth maps) for synthetic data, which is often more accurate than manual annotation of real data."}),"\n",(0,i.jsx)(e.h3,{id:"6-reality-gap-mitigation",children:"6. Reality Gap Mitigation"}),"\n",(0,i.jsx)(e.p,{children:"Strategies for reducing the differences between synthetic and real data to improve model transfer performance."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Synthetic Data Generation Pipeline:\n\n[Procedural Scene] \u2192 [Physics Simulation] \u2192 [Rendering Engine] \u2192 [Synthetic Images] \u2192 [Ground Truth]\n     Generation           & Lighting         & Post-Processing        & Labels        Annotations\n         \u2191                    \u2191                    \u2191                      \u2191                \u2191\n[Parameters] \u2190\u2192 [Objects] \u2190\u2192 [Materials] \u2190\u2192 [Cameras] \u2190\u2192 [Post-Process] \u2190\u2192 [Annotation]\n"})}),"\n",(0,i.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(e.h3,{id:"example-isaac-sim-synthetic-data-generation",children:"Example: Isaac Sim Synthetic Data Generation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.synthetic_utils.sensors import Camera, AnnotationCamera\nimport numpy as np\nimport cv2\nimport os\n\nclass SyntheticDataGenerator:\n    def __init__(self, output_dir="synthetic_data", num_samples=1000):\n        self.output_dir = output_dir\n        self.num_samples = num_samples\n\n        # Create output directories\n        os.makedirs(os.path.join(output_dir, "images"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, "labels"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, "depth"), exist_ok=True)\n\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Create synthetic data helper\n        self.sd_helper = SyntheticDataHelper()\n\n        # Add a camera for data generation\n        self.camera = Camera(\n            prim_path="/World/Camera",\n            position=np.array([0.0, 0.0, 1.0]),\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add annotation camera for segmentation\n        self.annotation_camera = AnnotationCamera(\n            prim_path="/World/AnnotationCamera",\n            position=np.array([0.0, 0.0, 1.0]),\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n    def generate_scene_variations(self):\n        """Generate random scene configurations"""\n        # Randomize lighting\n        light_intensity = np.random.uniform(500, 2000)\n        # Randomize object positions\n        object_x = np.random.uniform(-2.0, 2.0)\n        object_y = np.random.uniform(-2.0, 2.0)\n        # Randomize materials and textures\n        material_roughness = np.random.uniform(0.1, 0.9)\n\n        return {\n            "light_intensity": light_intensity,\n            "object_position": (object_x, object_y),\n            "material_roughness": material_roughness\n        }\n\n    def capture_synthetic_data(self, sample_idx):\n        """Capture synthetic data for a single sample"""\n        # Generate scene variation\n        scene_config = self.generate_scene_variations()\n\n        # Apply scene configuration\n        # (In practice, this would modify the stage with the new parameters)\n\n        # Step the simulation to update the scene\n        self.world.step(render=True)\n\n        # Capture RGB image\n        rgb_data = self.sd_helper.get_rgb_data(self.camera)\n        rgb_image = rgb_data["data"]\n\n        # Capture segmentation labels\n        seg_data = self.sd_helper.get_semantic_segmentation(self.annotation_camera)\n        seg_image = seg_data["data"]\n\n        # Capture depth data\n        depth_data = self.sd_helper.get_depth_data(self.camera)\n        depth_image = depth_data["data"]\n\n        # Save images\n        cv2.imwrite(\n            os.path.join(self.output_dir, "images", f"image_{sample_idx:06d}.png"),\n            cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n        )\n\n        cv2.imwrite(\n            os.path.join(self.output_dir, "labels", f"label_{sample_idx:06d}.png"),\n            seg_image\n        )\n\n        cv2.imwrite(\n            os.path.join(self.output_dir, "depth", f"depth_{sample_idx:06d}.png"),\n            (depth_image * 1000).astype(np.uint16)  # Scale depth for 16-bit storage\n        )\n\n        # Save metadata\n        metadata = {\n            "sample_idx": sample_idx,\n            "scene_config": scene_config,\n            "image_shape": rgb_image.shape,\n            "timestamp": self.world.current_time_step_index\n        }\n\n        return metadata\n\n    def generate_dataset(self):\n        """Generate the complete synthetic dataset"""\n        print(f"Generating {self.num_samples} synthetic data samples...")\n\n        metadata_list = []\n\n        for i in range(self.num_samples):\n            # Capture synthetic data\n            metadata = self.capture_synthetic_data(i)\n            metadata_list.append(metadata)\n\n            # Print progress\n            if (i + 1) % 100 == 0:\n                print(f"Generated {i + 1}/{self.num_samples} samples")\n\n        # Save metadata\n        import json\n        with open(os.path.join(self.output_dir, "metadata.json"), "w") as f:\n            json.dump(metadata_list, f, indent=2)\n\n        print(f"Synthetic dataset generation completed. Output saved to {self.output_dir}")\n\ndef main():\n    # Create synthetic data generator\n    generator = SyntheticDataGenerator(\n        output_dir="robotics_synthetic_dataset",\n        num_samples=500  # Adjust based on computational resources\n    )\n\n    # Generate dataset\n    generator.generate_dataset()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"example-domain-randomization-configuration",children:"Example: Domain Randomization Configuration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom dataclasses import dataclass\nfrom typing import Tuple\n\n@dataclass\nclass DomainRandomizationConfig:\n    """Configuration for domain randomization parameters"""\n\n    # Lighting randomization\n    light_intensity_range: Tuple[float, float] = (300.0, 2000.0)\n    light_color_temperature_range: Tuple[float, float] = (3000.0, 8000.0)\n    light_position_jitter: float = 0.5\n\n    # Material properties randomization\n    albedo_range: Tuple[float, float] = (0.1, 1.0)\n    roughness_range: Tuple[float, float] = (0.05, 0.95)\n    metallic_range: Tuple[float, float] = (0.0, 0.2)\n\n    # Camera parameters randomization\n    camera_fov_range: Tuple[float, float] = (30.0, 90.0)  # degrees\n    camera_position_jitter: float = 0.1\n    camera_rotation_jitter: float = 5.0  # degrees\n\n    # Environmental randomization\n    background_color_range: Tuple[float, float] = (0.0, 1.0)\n    fog_density_range: Tuple[float, float] = (0.0, 0.1)\n    texture_scale_range: Tuple[float, float] = (0.5, 2.0)\n\nclass DomainRandomizer:\n    """Handles domain randomization for synthetic data generation"""\n\n    def __init__(self, config: DomainRandomizationConfig):\n        self.config = config\n\n    def randomize_lighting(self):\n        """Generate randomized lighting parameters"""\n        return {\n            "intensity": np.random.uniform(*self.config.light_intensity_range),\n            "color_temperature": np.random.uniform(*self.config.light_color_temperature_range),\n            "position_jitter": np.random.uniform(-self.config.light_position_jitter,\n                                                self.config.light_position_jitter, 3)\n        }\n\n    def randomize_materials(self):\n        """Generate randomized material parameters"""\n        return {\n            "albedo": np.random.uniform(*self.config.albedo_range, 3),\n            "roughness": np.random.uniform(*self.config.roughness_range),\n            "metallic": np.random.uniform(*self.config.metallic_range)\n        }\n\n    def randomize_camera(self):\n        """Generate randomized camera parameters"""\n        return {\n            "fov": np.random.uniform(*self.config.camera_fov_range),\n            "position_jitter": np.random.uniform(-self.config.camera_position_jitter,\n                                                self.config.camera_position_jitter, 3),\n            "rotation_jitter": np.random.uniform(-self.config.camera_rotation_jitter,\n                                                self.config.camera_rotation_jitter, 3)\n        }\n\n    def randomize_environment(self):\n        """Generate randomized environmental parameters"""\n        return {\n            "background_color": np.random.uniform(*self.config.background_color_range, 3),\n            "fog_density": np.random.uniform(*self.config.fog_density_range),\n            "texture_scale": np.random.uniform(*self.config.texture_scale_range)\n        }\n\n    def get_randomization_params(self):\n        """Get all randomized parameters for a single sample"""\n        return {\n            "lighting": self.randomize_lighting(),\n            "materials": self.randomize_materials(),\n            "camera": self.randomize_camera(),\n            "environment": self.randomize_environment()\n        }\n\n# Example usage\nconfig = DomainRandomizationConfig()\nrandomizer = DomainRandomizer(config)\n\n# Generate randomization parameters for a sample\nparams = randomizer.get_randomization_params()\nprint(f"Randomized parameters: {params}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"example-synthetic-data-quality-evaluation",children:"Example: Synthetic Data Quality Evaluation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom skimage.metrics import structural_similarity as ssim\nfrom scipy.spatial.distance import cosine\nimport cv2\n\nclass SyntheticDataQualityEvaluator:\n    """Evaluates the quality of synthetic data compared to real data"""\n\n    def __init__(self):\n        pass\n\n    def evaluate_image_quality(self, synthetic_img, real_img):\n        """Evaluate image quality metrics between synthetic and real images"""\n        # Convert to grayscale for SSIM calculation\n        if len(synthetic_img.shape) == 3:\n            syn_gray = cv2.cvtColor(synthetic_img, cv2.COLOR_RGB2GRAY)\n            real_gray = cv2.cvtColor(real_img, cv2.COLOR_RGB2GRAY)\n        else:\n            syn_gray = synthetic_img\n            real_gray = real_img\n\n        # Calculate SSIM\n        ssim_score = ssim(syn_gray, real_gray)\n\n        # Calculate MSE\n        mse = np.mean((synthetic_img - real_img) ** 2)\n\n        # Calculate PSNR\n        if mse == 0:\n            psnr = float(\'inf\')\n        else:\n            max_pixel = 255.0\n            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n\n        return {\n            "ssim": ssim_score,\n            "mse": mse,\n            "psnr": psnr\n        }\n\n    def evaluate_distribution_similarity(self, synthetic_features, real_features):\n        """Evaluate how similar the feature distributions are"""\n        # Calculate cosine similarity between mean feature vectors\n        syn_mean = np.mean(synthetic_features, axis=0)\n        real_mean = np.mean(real_features, axis=0)\n\n        cosine_sim = 1 - cosine(syn_mean, real_mean)\n\n        # Calculate statistical moments comparison\n        syn_std = np.std(synthetic_features, axis=0)\n        real_std = np.std(real_features, axis=0)\n\n        std_ratio = np.mean(syn_std / (real_std + 1e-8))  # Avoid division by zero\n\n        return {\n            "cosine_similarity": cosine_sim,\n            "std_ratio": std_ratio\n        }\n\n    def evaluate_annotation_quality(self, synthetic_ann, real_ann):\n        """Evaluate quality of synthetic annotations compared to real"""\n        # Calculate IoU for segmentation masks\n        intersection = np.logical_and(synthetic_ann, real_ann)\n        union = np.logical_or(synthetic_ann, real_ann)\n\n        iou = np.sum(intersection) / (np.sum(union) + 1e-8)\n\n        # Calculate pixel accuracy\n        pixel_acc = np.mean(synthetic_ann == real_ann)\n\n        return {\n            "iou": iou,\n            "pixel_accuracy": pixel_acc\n        }\n\n# Example usage\nevaluator = SyntheticDataQualityEvaluator()\n\n# Example synthetic and real images (in practice, these would come from your dataset)\nsynthetic_example = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\nreal_example = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n# Evaluate image quality\nimg_quality = evaluator.evaluate_image_quality(synthetic_example, real_example)\nprint(f"Image quality metrics: {img_quality}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"practical-notes",children:"Practical Notes"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use domain randomization to improve synthetic-to-real transfer performance"}),"\n",(0,i.jsx)(e.li,{children:"Ensure synthetic data covers the full range of expected real-world conditions"}),"\n",(0,i.jsx)(e.li,{children:"Validate synthetic data quality against real data distributions"}),"\n",(0,i.jsx)(e.li,{children:"Consider computational requirements for large-scale synthetic data generation"}),"\n",(0,i.jsx)(e.li,{children:"Implement proper data versioning and tracking for synthetic datasets"}),"\n",(0,i.jsx)(e.li,{children:"Combine synthetic and real data for optimal model training"}),"\n",(0,i.jsx)(e.li,{children:"Monitor for artifacts or unrealistic elements in synthetic data"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Synthetic data generation has become a crucial component of modern robotics development, providing unlimited training data with perfect annotations while reducing the cost and complexity of real-world data collection. By leveraging simulation environments like Isaac Sim, robotics developers can create diverse, high-quality datasets that improve the robustness and generalization of their machine learning models. The key to successful synthetic data generation lies in proper domain randomization, quality evaluation, and validation against real-world performance."}),"\n",(0,i.jsx)(e.h2,{id:"glossary",children:"Glossary"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Synthetic Data"}),": Artificially generated data that mimics real-world observations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Domain Randomization"}),": Technique of randomizing simulation parameters to improve real-world transfer"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Procedural Generation"}),": Algorithmic creation of content rather than manual design"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Physics-Based Rendering"}),": Rendering that simulates physical light behavior for realism"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Ground Truth"}),": Accurate reference data used for training and evaluation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reality Gap"}),": Differences between synthetic and real-world data that affect model performance"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Annotation"}),": Labels or metadata associated with data samples"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"SSIM"}),": Structural Similarity Index Measure for image quality evaluation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"PSNR"}),": Peak Signal-to-Noise Ratio for image quality evaluation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"IoU"}),": Intersection over Union for segmentation quality evaluation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feature Distribution"}),": Statistical properties of data representations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Data Augmentation"}),": Techniques to artificially increase dataset diversity"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"quick-quiz",children:"Quick Quiz"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What is the primary advantage of synthetic data generation in robotics?\nA) Lower computational requirements\nB) Unlimited data with perfect annotations\nC) Simpler algorithms\nD) Reduced sensor requirements"}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What is domain randomization used for?\nA) Randomizing network protocols\nB) Improving synthetic-to-real transfer by randomizing simulation parameters\nC) Randomizing robot hardware\nD) Randomizing user interfaces"}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Which metric is commonly used to evaluate image quality between synthetic and real images?\nA) F1 Score\nB) Precision\nC) SSIM (Structural Similarity Index Measure)\nD) Recall"}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What does IoU stand for in the context of synthetic data evaluation?\nA) Input over Unit\nB) Intersection over Union\nC) Internal over Underlying\nD) Index of Understanding"}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'What is the "reality gap" in synthetic data generation?\nA) The physical gap between sensors\nB) Differences between synthetic and real-world data that affect model performance\nC) The time delay in simulation\nD) The cost difference between synthetic and real data'}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Answers:"})}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"B) Unlimited data with perfect annotations"}),"\n",(0,i.jsx)(e.li,{children:"B) Improving synthetic-to-real transfer by randomizing simulation parameters"}),"\n",(0,i.jsx)(e.li,{children:"C) SSIM (Structural Similarity Index Measure)"}),"\n",(0,i.jsx)(e.li,{children:"B) Intersection over Union"}),"\n",(0,i.jsx)(e.li,{children:"B) Differences between synthetic and real-world data that affect model performance"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(c,{...n})}):c(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>s});var t=a(6540);const i={},r=t.createContext(i);function o(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);